<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" 
  content="GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting">
  <meta property="og:title" content="GauU-Scene: A Large Scene Reconstruction Benchmark Using Highly Accurate Point Cloud"/>
  <meta property="og:description" content="  Our dataset is now publicly available via the link provided above. 
  Our enlarged dataset is divided into six main parts. The first 
  part is the top portion of this graph, referred to as SZIIT (The 
  Shenzhen Institute of Information Technology). The second row is 
  called the Lower Campus, an abbreviation for the Chinese University
   of Hong Kong, Lower Campus. The third row displays the Upper Campus
    of CUHKSZ, and the SMBU (Shenzhen MSU-BIT University) Campus. As 
    for the last row, it contains two auxiliary residential areas 
    named He Ao Village and LFLS(Longgang Foreign language school) We utilized highly accurate LiDAR 
    to collect the dataset, covering a range of more than 1.5 km^2. To 
    view the dataset from different angles, one can use the embedded 
    Youtube video provided. We store the dataset in Ply format on the 
    OneDrive share point link, with coordinates in WGS 84/ UTM zone 50N: 
    EPSG:32650 geographic standard. To further facilitate usage in computer 
    vision and graphics, we will also provide COLMAP datasets and aligned point clouds."/>
  <meta property="og:url" content="https://saliteta.github.io/CUHKSZ_SMBU/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static\images\GauU_V2_Dataset_Overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  
  
  <meta name="twitter:title" content="GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting">
  <meta name="twitter:description" content="GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static\images\GauU_V2_Dataset_Overview.png">
  <meta name="twitter:card" content="">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Gaussian Splatting, Butian, Large Scale, Dataset, Xiong, GauU-Scene, GauU-Scene V2, 3D Reconstruction,">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>GauU-Scene</title>
  <link rel="icon" type="image/x-icon" href="static\images\webpage_raw.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">

  <!-- Optional: Bootstrap JS and its dependencies (jQuery and Popper) -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<style>
    img {
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 90%; /* Adjust this value as needed */
    }

    h2.subtitle.has-text-centered {
        margin-left: 30%; /* Adjust the margin as needed */
        margin-right: 30%; /* Adjust the margin as needed */
        text-align: center;
    }
</style>


  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GauUscene</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="dwadsdawda" target="_blank">Anonymous Authors</a><sup>*</sup>,</span>

                  <div class="is-size-5 publication-authors">
                    <!--The Chinese University of Hong Kong, Shenzhen-->
                    <span class="author-block">Anonymous Institution<br>IJCAI 2024</span>
                  </div>


                    <!-- Supplementary PDF link -->
                  <span class="link-block">
                      <a href="static/pdfs/data_collection_protocal.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!--Dataset link -->
                  <span class="link-block">
                    <a href="static/pdfs/GauUscene_license.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.14032" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>GauU-Scene V1</span>
                </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.04880" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>GauU-Scene V2</span>
                </a>
                </span>

                 <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/saliteta/lidar_SfM_alignment" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel large-scale scene reconstruction benchmark
             using the newly developed 3D representation approach, Gaussian
              Splatting, on our expansive U-Scene dataset. U-Scene encompasses
               over one and a half square kilometers in the first version and
                3.5 square kilometers in the second version, featuring a comprehensive
                 RGB dataset coupled with LiDAR ground truth. The dataset 
                 size is continually growing and now includes 6 scenarios. 
                 For data acquisition, we employed the Matrix 300 drone 
                 equipped with the high-accuracy Zenmuse L1 LiDAR, enabling 
                 precise rooftop data collection. The detailed data acquisition
                  protocol is appended in the supplementary material. 
                  It contains drone assembly, controller path planning, 
                  controller assembly, safety and protection, RTK Help, 
                  Drone Data Post-Processing, and many other details. 
                  U-Scene, developed under the auspices of the Chinese University of Hong Kong, 
                  Shenzhen MSU-BIT University, SZIIT, and auxiliary residential area, offers a 
                  unique blend of urban and academic environments for advanced spatial analysis. 
                  Our evaluation of U-Scene with Gaussian Splatting includes a detailed 
                  analysis across various novel viewpoints. We also juxtapose these 
                  results with those derived from our accurate point cloud dataset, 
                  highlighting significant differences that underscore the importance and innovation of our work.
          </p>
          <div class="column is-four-fifths">
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<img src="static/images/dataset_preview.png" alt="MY ALT TEXT"  style="height: 600px;"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
  Our dataset is now publicly available via the link provided above. 
  Our enlarged dataset is divided into six main parts. The first 
  part is the top portion of this graph, referred to as SZIIT (The 
  Shenzhen Institute of Information Technology). The second row is 
  called the Lower Campus, an abbreviation for the Chinese University
   of Hong Kong, Lower Campus. The third row displays the Upper Campus
    of CUHKSZ, and the SMBU (Shenzhen MSU-BIT University) Campus. As 
    for the last row, it contains two auxiliary residential areas 
    named He Ao Village and LFLS(Longgang Foreign language school) We utilized highly accurate LiDAR 
    to collect the dataset, covering a range of more than 1.5 km^2. To 
    view the dataset from different angles, one can use the embedded 
    Youtube video provided. We store the dataset in Ply format on the 
    OneDrive share point link, with coordinates in WGS 84/ UTM zone 50N: 
    EPSG:32650 geographic standard. To further facilitate usage in computer 
    vision and graphics, we will also provide COLMAP datasets and aligned point clouds.
    </div>
  </div>
</div>

<img src="static/images/Different_Reflectivity.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      Our dataset provides essential information for quality control and multi-modal analysis and visualization. By using
      professional tools such as DJI Terra, one can observe three important properties critical for quality control: Reflectivity, Height,
      and Return. Graph (a) in this figure illustrates reflectivity, which measures the amount of light reflected back to the LiDAR
      sensor from surfaces or objects. Meanwhile, height, shown in graph (b), represents the building's altitude relative to the drone's
      takeoff altitude. The return, presented in graph (c), indicates the number of light returns detected by the LiDAR. Since our
      analysis filters out all data except those with at least two returns, moving objects, represented by red dots, will be excluded.
      More visualization results can be explored in our dataset or in the supplementary materials.
    </div>
  </div>
</div>

<img src="static/images/comaprison.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  </div>
</div>

<img src="static/images/method_comparision.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  </div>
</div>

<img src="static/images/geometry.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  </div>
</div>

<img src="static/images/Methods_Overview.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      1: The dataset prepared for input into the neural field
      and Gaussian Splatting typically consists of camera positions
      and images in COLMAP format. The Structure from Motion
      (SfM) algorithm implemented in COLMAP initializes camera
      positions randomly, which may not align with LiDAR data
      in WGS 84 coordinates. This discrepancy poses a significant
      challenge for geometric alignment measurement and multimodal fusion algorithms. When inputs are in two different
      coordinate systems, further validation becomes impractical.
      To address this, we propose a straightforward yet effective
      method for statistical scale matching to align LiDAR point
      clouds with camera positions. This approach is crucial for the
      construction of our dataset.
    </div>
  </div>
</div>

<img src="static/images/Block Aquisition.png" alt="MY ALT TEXT"/>
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <div class="content has-text-justified">
      This figure shows the design of the drone routing path. The white and orange dots represent the positions where the
      drone took pictures. The overall path for a scene is shown in Graph (a), which is composed of several micro-blocks. One such
      micro-block, highlighted in orange, is detailed in Graph (a). Zooming into this orange micro-block reveals Figure (b). The total
      path length of each micro-block is limited by the battery life of the DJI Matrice 300, as well as the power consumption of the
      LiDAR in windy conditions. For safety reasons, each micro-block typically covers an area of 350 x 350 square meters. Each
      micro-block has five routing paths, providing different angles for photography, as illustrated in Figure (c). The first routing
      path offers a Bird's Eye View (BEV), while the subsequent four paths alter the camera's orientation by 45 degrees towards the
      horizontal plane. These four paths' camera orientations are forward, backward, rightward, and leftward, respectively.
    </div>
  </div>
</div>



<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Point Cloud Data Preview</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/video/hav.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/video/lfls.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/video/lower_campus.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/video/smbu.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/video/sziit.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/video/upper_campus.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->



<div class="container">
<div class="table-container">
  <div class="content has-text-justified">
    <h2 class="title is-3">Point Cloud Data Quantitative Scale</h2>
  </div>
    <table>
    <thead>
      <tr>
        <th></th>
        <th>Size in km<sup>2</sup></th>
        <th>Image Number</th>
        <th>Points Number</th>
        <th>DJI Raw Data Size in GB</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Lower Campus</td>
        <td>1.020267099</td>
        <td>670</td>
        <td>79,767,884</td>
        <td>12.5</td>
      </tr>
      <tr>
        <td>Upper Campus</td>
        <td>0.923096497</td>
        <td>715</td>
        <td>94,218,901</td>
        <td>13.5</td>
      </tr>
      <tr>
        <td>SMBU</td>
        <td>0.908184476</td>
        <td>563</td>
        <td>283,31,405</td>
        <td>16.2</td>
      </tr>
      <tr>
        <td>SZIIT</td>
        <td>1.557606058</td>
        <td>1215</td>
        <td>58,979,628</td>
        <td>22.3</td>
      </tr>
      <tr>
        <td>HAV</td>
        <td>0.815080080</td>
        <td>424</td>
        <td>26,759,799</td>
        <td>7.8</td>
      </tr>
      <tr>
        <td>LFLS</td>
        <td>1.466664729</td>
        <td>1106</td>
        <td>98,547,710</td>
        <td>19.8</td>
      </tr>
      <tr>
        <td>Total</td>
        <td>6.668</td>
        <td>4693</td>
        <td>627,500,327</td>
        <td>92.1</td>
      </tr>
    </tbody>
  </table>
</div>
</div>



<div class="container">
  <div class="title is-3">Download Dataset</div>
  <p>
    If you are interested in the data, please first obtained the license at the dataset link provided above
    and send email to the correspondance according to the guidance written in the license. 
  </p>
</div>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>

      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
